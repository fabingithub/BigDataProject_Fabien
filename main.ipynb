{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import codecs, json\n",
    "import unicodedata\n",
    "import collections\n",
    "import datetime\n",
    "import math\n",
    "import re\n",
    "import pickle\n",
    "# pip install Unidecode  <OR> conda install Unidecode\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Variables and what they mean\n",
    "#   duplicate_dict = all duplicates\n",
    "#   dict_removed_single_entries = all duplicates but removed single entries\n",
    "#   duplicate_ids_kept = array of all ids from dict_removed_single_entries\n",
    "#   dict_duplicate_compare_team_members = map values from teams to einstaklingsid\n",
    "#   dict_name_entries = map values from member down one step (name->birthday->values) now (name+birthday->values)\n",
    "#   dict_einstaklingar_teammember_info = map teammember values to correct key in einstaklingsid\n",
    "#   not_the_same_person = when two players are playing in different teams at the same time then they clearly are not the same person"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all csv files\n",
    "domarar = pd.read_csv('csv/blak-domarar.csv', sep=';', header=0)\n",
    "einstaklingar = pd.read_csv('csv/blak-einstaklingar.csv', sep=';', header=0)\n",
    "forsvarsmenn = pd.read_csv('csv/blak-forsvarsmenn.csv', sep=';', header=0)\n",
    "lid = pd.read_csv('csv/blak-lid.csv', sep=';', header=0)\n",
    "lidimoti = pd.read_csv('csv/blak-lidimoti.csv', sep=';', header=0)\n",
    "lidsmenn = pd.read_csv('csv/blak-lidsmenn.csv', sep=';', header=0)\n",
    "lidsstjorar = pd.read_csv('csv/blak-lidsstjorar.csv', sep=';', header=0)\n",
    "thjalfarar = pd.read_csv('csv/blak-thjalfarar.csv', sep=';', header=0)\n",
    "mot = pd.read_csv('csv/blak-mot.csv', sep=';', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all SyndarLids with an ID (SyndarlidID)\n",
    "# (the reason for not dropping using SyndarLid is because I don't trust that column to be inserted correctly with [0,1])\n",
    "lid = lid[lid['SyndarlidID'].isna()]\n",
    "# then dropping those two columns because we don't want virtual teams\n",
    "lid = lid.drop(columns=['SyndarLid', 'SyndarlidID'])\n",
    "\n",
    "# All duplicated birthdays\n",
    "duplicated_einstaklingar = einstaklingar[einstaklingar.duplicated(subset=['Nafn', 'Fdagur', 'Kyn'], keep=False)]\n",
    "duplicated_fdagur_kyn_einstaklingar = einstaklingar[einstaklingar.duplicated(subset=['Fdagur', 'Kyn'], keep=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update incorrect nafn and radnumer in lid table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "lidi = lid\n",
    "\n",
    "def isaRadNumber(x):\n",
    "    valid = ['0','1','2','3','4','5','6','7','8','9','a','b','c','d','e','f','A','B','C','D','E','F']\n",
    "    return x in valid\n",
    "    \n",
    "def correctRadNumbersFromEntries():\n",
    "    for index,row in lidi.iterrows():\n",
    "        radNumber = row['Radnumer']\n",
    "        id = row['LidID']\n",
    "        \n",
    "        if(not(isaRadNumber(radNumber))):\n",
    "            #print(\"needs to modify id \", id)\n",
    "            name = row['Nafn']\n",
    "            size = len(name.split())\n",
    "            club = ' '.join(name.split()[:max(1,(size-2))])#club name is first part\n",
    "            ordinal = name.split()[size-1]#last str\n",
    "            \n",
    "            if(len(ordinal) == 1 & isaRadNumber(ordinal)):# if name ends with a single char, used for radnumbers\n",
    "                lidi.at[index,'Radnumer'] = ordinal\n",
    "                lidi.at[index,'Nafn'] = club       \n",
    "\n",
    "# call def\n",
    "correctRadNumbersFromEntries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lidi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working on phone numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isBlank (mystring):\n",
    "    if (isinstance(mystring,float)):\n",
    "        return True\n",
    "    else:\n",
    "        return not (mystring and mystring.strip())\n",
    "\n",
    "einstak = einstaklingar\n",
    "\n",
    "def validChar(x):\n",
    "    valid = ['0','1','2','3','4','5','6','7','8','9']\n",
    "    return x in valid\n",
    "\n",
    "\n",
    "def convertPhoneEntry(string):\n",
    "    s = \"\"\n",
    "    for e in string:\n",
    "        if(validChar(e)):\n",
    "            s += e\n",
    "    return s\n",
    "            \n",
    "    \n",
    "def movePhoneFieldsEntries():\n",
    "    for index,row in einstak.iterrows():\n",
    "        adr1 = row['Simi1']\n",
    "        adr2 = row['Simi2']\n",
    "        adr3 = row['Simi3']\n",
    "        a,b,c = False,False,False\n",
    "        \n",
    "        if (isBlank(adr1)):\n",
    "            a=True\n",
    "        if (isBlank(adr2)):\n",
    "            b=True\n",
    "        if (isBlank(adr3)):\n",
    "            c=True\n",
    "            \n",
    "        if(a & b & (not c) ):\n",
    "            einstak.at[index,'Simi1'] = convertPhoneEntry(adr3)\n",
    "            einstak.at[index,'Simi3'] = adr1#Nan\n",
    "        elif(a & (not b) &  c):\n",
    "            einstak.at[index,'Simi1'] = convertPhoneEntry(adr2)\n",
    "            einstak.at[index,'Simi2'] = adr1#Nan\n",
    "        elif(a & (not b) & (not c)):\n",
    "            einstak.at[index,'Simi1'] = convertPhoneEntry(adr2)\n",
    "            einstak.at[index,'Simi2'] = convertPhoneEntry(adr3)\n",
    "            einstak.at[index,'Simi3'] = adr1#Nan\n",
    "        elif((not a) & b & (not c)):\n",
    "            einstak.at[index,'Simi1'] = convertPhoneEntry(adr1)\n",
    "            einstak.at[index,'Simi2'] = convertPhoneEntry(adr3)\n",
    "            einstak.at[index,'Simi3'] = adr2#Nan\n",
    "        else:\n",
    "            if(not a):\n",
    "                einstak.at[index,'Simi1'] = convertPhoneEntry(adr1)\n",
    "            if(not b):\n",
    "                einstak.at[index,'Simi2'] = convertPhoneEntry(adr2)\n",
    "            if(not c):\n",
    "                einstak.at[index,'Simi3'] = convertPhoneEntry(adr3)\n",
    "            \n",
    "# call def\n",
    "movePhoneFieldsEntries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "einstak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter duplicates by name and birthday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add all entries that have duplicated birthdays, then filter that to first_name->birthday-><people entries>\n",
    "duplicate_dict = defaultdict(dict)\n",
    "\n",
    "def create_duplicated_entries(duplication_array):\n",
    "    for index, row in duplication_array.iterrows():\n",
    "        full_name = row['Nafn']\n",
    "        first_name = full_name.split()[0]\n",
    "        last_name = full_name.split()[::-1][0]\n",
    "\n",
    "        # make name lowercase\n",
    "        first_name_lowercase = first_name.lower()\n",
    "        last_name_lowercase = last_name.lower()\n",
    "\n",
    "        # encode icelandic letters to english\n",
    "        first_name_to_english = unidecode.unidecode(first_name_lowercase)\n",
    "        last_name_to_english =  unidecode.unidecode(last_name_lowercase)\n",
    "\n",
    "        # split birthday into year month and day and ignore second part (sec, min, hour)\n",
    "        if (last_name_to_english != first_name_to_english):\n",
    "            Fdagur_date = row['Fdagur'].split()[0] + \"+\" + last_name_to_english\n",
    "        else:\n",
    "            Fdagur_date = row['Fdagur'].split()[0] + \"+\" + \"<MISSING>\"\n",
    "\n",
    "        if first_name_to_english in duplicate_dict.keys():\n",
    "            if Fdagur_date in duplicate_dict[first_name_to_english].keys():\n",
    "                #if first name and Fdagur (birthday) exist in dict then append to that key (birthday)\n",
    "                duplicate_dict[first_name_to_english][Fdagur_date].append(row.values)\n",
    "            else:\n",
    "                #if first name exists but Fdagur (birthday) does not exist in dict\n",
    "                duplicate_dict[first_name_to_english][Fdagur_date] = [row.values]\n",
    "        else:\n",
    "            #if Fdagur (birthday) does not exist in dict\n",
    "            duplicate_dict[first_name_to_english][Fdagur_date] = [row.values]\n",
    "            \n",
    "# call def\n",
    "create_duplicated_entries(duplicated_fdagur_kyn_einstaklingar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove all single birthday entries (since that is not a duplicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all single birthday entries that are not duplicates\n",
    "dict_removed_single_entries = defaultdict(dict)\n",
    "\n",
    "def remove_single_entries(duplication_array):\n",
    "    for key, values in duplication_array.items():\n",
    "        # key = nafn ('ludvik')\n",
    "        for birthday, arrays in dict(values).items():\n",
    "            # only get duplicates that there exists 2 or more entries for a birthday\n",
    "            if(len(arrays) > 1):\n",
    "                # used for when joining teams table\n",
    "                if key in dict_removed_single_entries.keys():\n",
    "                    if birthday in dict_removed_single_entries[key].keys():\n",
    "                        dict_removed_single_entries[key][birthday].append(arrays)\n",
    "                    else:\n",
    "                        #if first name exists but Fdagur (birthday) does not exist in dict\n",
    "                        dict_removed_single_entries[key][birthday] = arrays\n",
    "                else:\n",
    "                    dict_removed_single_entries[key][birthday] = arrays\n",
    "\n",
    "# call def\n",
    "remove_single_entries(duplicate_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_removed_single_entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map (name+birthday+lastname-> unique id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_name_entries = {}\n",
    "\n",
    "def identifier_map_unique_ids(duplication_array):\n",
    "    for key, value in duplication_array.items():\n",
    "        #get key and arrays for each person\n",
    "        for birthday, arrays in dict(value).items():\n",
    "            #get each array for person\n",
    "            new_key = key +\"+\"+ birthday\n",
    "            for item in arrays:\n",
    "                if new_key in dict_name_entries.keys():\n",
    "                    dict_name_entries[new_key].append(item[0])\n",
    "                else:\n",
    "                    dict_name_entries[new_key] = [item[0]]\n",
    "# call def\n",
    "identifier_map_unique_ids(dict_removed_single_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_name_entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get all ids that exists in duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all ids in dict_removed_single_entries\n",
    "duplicate_ids_kept = []\n",
    "def get_duplication_keys(duplication_array):\n",
    "    for key, values in duplication_array.items():\n",
    "        # key = nafn ('ludvik')\n",
    "        for birthday, arrays in dict(values).items():\n",
    "            for item in arrays:\n",
    "                duplicate_ids_kept.append(item[0])\n",
    "\n",
    "# call def\n",
    "get_duplication_keys(dict_removed_single_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_ids_kept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map Lid table values to einstaklingsID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if two names are the same person\n",
    "dict_duplicate_compare_team_members = defaultdict(dict)\n",
    "\n",
    "def team_member_entries(duplication_array):\n",
    "    for index, row in duplication_array.iterrows():\n",
    "        ids = row[\"EinstID\"]\n",
    "        if ids in duplicate_ids_kept:\n",
    "            # now we only view ids that exist for duplicated people\n",
    "            #print(ids)\n",
    "            if ids in dict_duplicate_compare_team_members.keys():\n",
    "                dict_duplicate_compare_team_members[ids].append(row.values)\n",
    "            else:\n",
    "                dict_duplicate_compare_team_members[ids] = [row.values]\n",
    "\n",
    "# call def\n",
    "team_member_entries(lidsmenn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_duplicate_compare_team_members"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EinstaklingsID connect to his data in teams table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_einstaklingar_teammember_info = {}\n",
    "\n",
    "def connect_members_to_team_data(duplication_array):\n",
    "    for key, value in duplication_array.items():\n",
    "        #print(\"<key>\" + str(key) + \" <value> \" + str(value))\n",
    "        for item in value:\n",
    "            #print(item)\n",
    "            if item in dict_duplicate_compare_team_members.keys():\n",
    "                #print(\"<key>\" + str(key) + \" <item> \" + str(item))\n",
    "                for compare_arrays in dict_duplicate_compare_team_members[item]:\n",
    "                    mot_id = compare_arrays[0]\n",
    "                    lid_id = compare_arrays[1]\n",
    "                    player_id = compare_arrays[2]\n",
    "                    date = compare_arrays[3]\n",
    "                    date_played = compare_arrays[3].split()[0]\n",
    "\n",
    "                    temp = (str(date) + \" \" + str(mot_id) + \" \" + str(lid_id) + \" \" + str(player_id))   \n",
    "                    if key in dict_einstaklingar_teammember_info.keys():\n",
    "                        dict_einstaklingar_teammember_info[key].append(temp)\n",
    "                    else:\n",
    "                        dict_einstaklingar_teammember_info[key] = [temp]\n",
    "\n",
    "# call def\n",
    "connect_members_to_team_data(dict_name_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_einstaklingar_teammember_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find if a potential duplicated person played two games at the same time in different teams (then he is not a duplication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_the_same_person = {}\n",
    "most_likely_same_person = {}\n",
    "\n",
    "def find_duplicates(key, nums):\n",
    "    num_set = set()\n",
    "    duplicates = set()\n",
    "    no_duplicate = -1\n",
    "    sorted_nums = sorted(nums)\n",
    "    last_array_entry = \"\"\n",
    "    for i in range(len(sorted_nums)):\n",
    "        for j in range(i+1, len(sorted_nums)):\n",
    "            \n",
    "            # team one split\n",
    "            #(str(date) + \" \" + str(mot_id) + \" \" + str(lid_id) +  str(player_id))   \n",
    "            \n",
    "            sort_1 = sorted_nums[i].split()\n",
    "            date_1 = sort_1[0]\n",
    "            mot_id_1 = sort_1[2]\n",
    "            team_id_1 = sort_1[3]\n",
    "            einstaklings_id_1 = sort_1[4]\n",
    "            date_time_str_1 = sort_1[0]+\" \"+sort_1[1]\n",
    "            date_time_obj_1 = datetime.datetime.strptime(date_time_str_1, '%Y-%m-%d %H:%M:%S.%f')\n",
    "            \n",
    "            # team two split\n",
    "            sort_2 = sorted_nums[j].split()\n",
    "            date_2 = sort_2[0]\n",
    "            mot_id_1 = sort_2[2]\n",
    "            team_id_2 = sort_2[3]\n",
    "            einstaklings_id_2 = sort_2[4]    \n",
    "            date_time_str_2 = sort_2[0]+\" \"+sort_2[1]\n",
    "            date_time_obj_2 = datetime.datetime.strptime(date_time_str_2, '%Y-%m-%d %H:%M:%S.%f')\n",
    "            \n",
    "            # time difference between these two entries\n",
    "            time_diff = (date_time_obj_2 - date_time_obj_1).total_seconds()/60\n",
    "            \n",
    "            match_length = 20\n",
    "            # There exist two record for erla with the same einstaklingsid but different teams (6286 and 6285)\n",
    "            # played 6.5 minutes apart\n",
    "            if((date_1 == date_2) and (time_diff < match_length) and (einstaklings_id_1 != einstaklings_id_2)):\n",
    "                combined = [sorted_nums[i], sorted_nums[j]]\n",
    "                if key in not_the_same_person.keys():\n",
    "                    not_the_same_person[key].append(combined)\n",
    "                else:\n",
    "                    not_the_same_person[key] = [combined]\n",
    "                    \n",
    "            else:\n",
    "                if last_array_entry != sorted_nums[i]:\n",
    "                    # Here are all the potential duplicates left after filtering\n",
    "                    #combined = [sorted_nums[i], sorted_nums[j]]\n",
    "                    if key in most_likely_same_person.keys():\n",
    "                        most_likely_same_person[key].append(sorted_nums[i])\n",
    "                    else:\n",
    "                        most_likely_same_person[key] = [sorted_nums[i]]\n",
    "                last_array_entry = sorted_nums[i]\n",
    "\n",
    "for key, value in dict_einstaklingar_teammember_info.items():\n",
    "    find_duplicates(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_the_same_person.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(not_the_same_person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_likely_same_person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(most_likely_same_person)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All ids that should be able to be merged together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_list = {}\n",
    "\n",
    "def people_abled_to_merge(duplication_array):\n",
    "    temp_arr = []\n",
    "    def merged_ids(key, nums):\n",
    "        for compare_arrays in nums:\n",
    "\n",
    "            sort_1 = compare_arrays.split()\n",
    "            einstaklings_id_1 = sort_1[4]\n",
    "\n",
    "            if key in merged_list.keys():\n",
    "                if einstaklings_id_1 not in temp_arr:\n",
    "                    #print(key)\n",
    "                    #print(einstaklings_id_1)\n",
    "                    merged_list[key].append(einstaklings_id_1)\n",
    "                    temp_arr.append(einstaklings_id_1)\n",
    "            else:\n",
    "                merged_list[key] = [einstaklings_id_1]\n",
    "                temp_arr.append(einstaklings_id_1)\n",
    "\n",
    "    for key, value in duplication_array.items():\n",
    "        merged_ids(key, value)\n",
    "        temp_arr = []\n",
    "\n",
    "# call def\n",
    "people_abled_to_merge(most_likely_same_person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_name_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FINAL STEP (run after everything is done):\n",
    "merge_suggestion_for_people = (str(merged_list))\n",
    "text_file = open(\"merge_suggestions.txt\", \"w\")\n",
    "text_file.write(merge_suggestion_for_people)\n",
    "text_file.close()\n",
    "\n",
    "pickle_obj = open(\"merge_suggestions.pickle\", \"wb\")\n",
    "pickle.dump(merged_list, pickle_obj)\n",
    "pickle_obj.close()\n",
    "\n",
    "#duplicated people put into it's own csv to be browsed later\n",
    "pd.DataFrame(duplicated_einstaklingar).to_csv(\"csv/new/duplicated-einstaklingar.csv\", encoding='utf-8-sig')\n",
    "\n",
    "#save as new csv inside csv/new\n",
    "pd.DataFrame(domarar).to_csv(\"csv/new/blak-domarar.csv\", encoding='utf-8-sig')\n",
    "pd.DataFrame(einstak).to_csv(\"csv/new/blak-einstaklingar.csv\", encoding='utf-8-sig') # Phones have a good format now\n",
    "pd.DataFrame(forsvarsmenn).to_csv(\"csv/new/blak-forsvarsmenn.csv.csv\", encoding='utf-8-sig')\n",
    "pd.DataFrame(lidi).to_csv(\"csv/new/blak-lid.csv\", encoding='utf-8-sig') # lidi stands for lid_updated, more values in Radnumer\n",
    "pd.DataFrame(lidimoti).to_csv(\"csv/new/blak-lidimoti.csv\", encoding='utf-8-sig')\n",
    "pd.DataFrame(lidsmenn).to_csv(\"csv/new/blak-lidsmenn.csv\", encoding='utf-8-sig')\n",
    "pd.DataFrame(lidsstjorar).to_csv(\"csv/new/blak-lidsstjorar.csv\", encoding='utf-8-sig')\n",
    "pd.DataFrame(mot).to_csv(\"csv/new/blak-mot.csv\", encoding='utf-8-sig')\n",
    "pd.DataFrame(thjalfarar).to_csv(\"csv/new/blak-thjalfarar.csv\", encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
